"""High level orchestration for the municipal development plan pipeline.

This module implements the system orchestrator described in
``PSEUDOCODIGO_FLUJO_COMPLETO.md`` and the architectural guidelines in
``rules/METODOS/ARQUITECTURA_ORQUESTADOR_COREOGRAFO.md``.  The orchestrator is
responsible for coordinating the ten phases of the end-to-end workflow while
delegating all question-level execution to the coreographer subsystem.

Key design goals
----------------

* **Strict role boundaries** – the orchestrator never inspects or executes the
  DAG of methods for an individual question.  It only dispatches work to
  coreographers and aggregates their reported results.
* **Deterministic sequencing** – all phases are executed in the order defined
  by the pseudocode with explicit state transitions that can be inspected via
  :meth:`get_processing_status`.
* **Observability and fault tolerance** – timings, progress counters and error
  records are captured for every phase.  Critical failures are routed through
  :meth:`handle_critical_error` in alignment with the doctrine described in the
  architecture notes.

The module intentionally avoids importing component implementations (document
ingestion, scoring, aggregation, report assembly) at import time so that it can
be safely imported before those components are available.  Resolution happens
at call-time and produces actionable diagnostics when a dependency has not been
implemented yet.
"""

from __future__ import annotations

import concurrent.futures
import hashlib
import importlib
import json
import logging
import threading
import time
from collections import Counter
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from statistics import mean
from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple

logger = logging.getLogger(__name__)


EXPECTED_METHOD_TOTAL = 416


class ProcessingPhase(str, Enum):
    """Enumeration of the orchestration phases in execution order."""

    IDLE = "idle"
    VALIDATION = "validation"
    INGESTION = "ingestion"
    MICRO_EXECUTION = "micro_execution"
    SCORING = "scoring"
    DIMENSION_AGGREGATION = "dimension_aggregation"
    AREA_AGGREGATION = "area_aggregation"
    CLUSTER_AGGREGATION = "cluster_aggregation"
    MACRO_EVALUATION = "macro_evaluation"
    RECOMMENDATIONS = "recommendations"
    REPORT_ASSEMBLY = "report_assembly"
    OUTPUT_FORMATTING = "output_formatting"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class PhaseTiming:
    """Timing information for a single processing phase."""

    start_time: float
    end_time: Optional[float] = None

    def complete(self) -> None:
        """Mark the phase as finished and record its end time."""

        if self.end_time is None:
            self.end_time = time.time()

    @property
    def duration(self) -> Optional[float]:
        """Return the duration in seconds if the phase has completed."""

        if self.end_time is None:
            return None
        return self.end_time - self.start_time


@dataclass
class ProcessingMetrics:
    """Aggregated metrics generated during orchestration."""

    total_start: Optional[float] = None
    total_end: Optional[float] = None
    phase_timings: Dict[ProcessingPhase, PhaseTiming] = field(default_factory=dict)
    micro_question_durations: List[float] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

    def begin_phase(self, phase: ProcessingPhase) -> None:
        """Register the start timestamp for ``phase``."""

        self.phase_timings[phase] = PhaseTiming(start_time=time.time())

    def end_phase(self, phase: ProcessingPhase) -> None:
        """Finalize timing data for ``phase`` if it has started."""

        timing = self.phase_timings.get(phase)
        if timing:
            timing.complete()

    def record_micro_duration(self, duration: float) -> None:
        """Append a duration measurement for a micro question."""

        self.micro_question_durations.append(duration)


@dataclass
class ProcessingState:
    """Mutable state snapshot for the orchestrator."""

    current_phase: ProcessingPhase = ProcessingPhase.IDLE
    questions_completed: int = 0
    questions_total: int = 300
    last_progress_update: float = field(default_factory=time.time)
    active_futures: List[concurrent.futures.Future] = field(default_factory=list)
    preprocessed_document: Optional[Any] = None
    scored_results: Optional[List[Any]] = None
    dimension_scores: Optional[List[Any]] = None
    area_scores: Optional[List[Any]] = None
    cluster_scores: Optional[List[Any]] = None
    macro_score: Optional[Any] = None
    recommendations: Optional[List[Any]] = None
    complete_report: Optional[Any] = None
    outputs: Optional[Dict[str, Any]] = None


@dataclass
class ErrorResponse:
    """Response generated by :meth:`Orchestrator.handle_critical_error`."""

    decision: str
    reason: str
    completed_questions: int
    total_questions: int


class QuestionExecutor(Protocol):
    """Protocol implemented by the coreographer for executing a question."""

    def __call__(
        self,
        question_global: int,
        preprocessed_document: Any,
        monolith: Dict[str, Any],
        method_catalog: Dict[str, Any],
    ) -> Any:
        ...


@dataclass
class ChoreographerPool:
    """Wrapper for the executor handling concurrent micro questions."""

    executor: concurrent.futures.Executor
    max_workers: int

    def shutdown(self) -> None:
        """Shutdown the underlying executor."""

        self.executor.shutdown(wait=True)


def _load_json(path: Path) -> Dict[str, Any]:
    """Load JSON data from ``path`` and return a Python dictionary."""

    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def _compute_integrity_hash(payload: Dict[str, Any]) -> str:
    """Compute the canonical hash of a JSON payload."""

    canonical = json.dumps(payload, sort_keys=True, separators=(",", ":")).encode("utf-8")
    return hashlib.sha256(canonical).hexdigest()


class Orchestrator:
    """Central coordinator for the municipal development plan pipeline.

    Parameters
    ----------
    monolith_path:
        File path to ``questionnaire_monolith.json``.
    catalog_path:
        File path to ``metodos_completos_nivel3.json``.
    config:
        Optional dictionary overriding orchestration configuration.  Supported
        keys include ``max_workers`` (int), ``status_interval`` (float) and
        ``micro_question_timeout`` (float, seconds).
    question_executor_factory:
        Optional callable returning a :class:`QuestionExecutor`.  When omitted,
        the orchestrator tries to resolve ``coreographer.Coreographer`` and use
        its ``execute`` method, raising a descriptive error if the module is
        unavailable.
    """

    DEFAULT_CONFIG: Dict[str, Any] = {
        "max_workers": 24,
        "status_interval": 10.0,
        "micro_question_timeout": 30 * 60.0,  # 30 minutes
        "max_retries": 3,
        "retry_backoff_seconds": [0.0, 5.0, 15.0],
    }

    def __init__(
        self,
        monolith_path: str | Path = "questionnaire_monolith.json",
        catalog_path: str | Path = "rules/METODOS/metodos_completos_nivel3.json",
        config: Optional[Dict[str, Any]] = None,
        question_executor_factory: Optional[Callable[[Dict[str, Any]], QuestionExecutor]] = None,
    ) -> None:
        self.monolith_path = Path(monolith_path)
        self.catalog_path = Path(catalog_path)

        if not self.monolith_path.exists():
            raise FileNotFoundError(f"Questionnaire monolith not found: {self.monolith_path}")
        if not self.catalog_path.exists():
            raise FileNotFoundError(f"Method catalog not found: {self.catalog_path}")

        self.monolith: Dict[str, Any] = _load_json(self.monolith_path)
        self.method_catalog: Dict[str, Any] = _load_json(self.catalog_path)

        self.config = {**self.DEFAULT_CONFIG, **(config or {})}
        self.metrics = ProcessingMetrics()
        self.state = ProcessingState()
        self._lock = threading.Lock()

        self._question_executor_factory = question_executor_factory
        self._coreographer_cls: Optional[type] = None

        logger.info(
            "Orchestrator initialized with monolith %s and catalog %s",
            self.monolith_path,
            self.catalog_path,
        )

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def process_document(self, pdf_path: str | Path) -> Dict[str, Any]:
        """Execute the full end-to-end pipeline for ``pdf_path``.

        This method is the single public entry point of the orchestrator.  It
        advances through each processing phase, recording metrics and updating
        the internal state machine.  Component-specific work is delegated to the
        specialised modules described in the architecture documentation.
        """

        self.metrics.total_start = time.time()
        try:
            self._start_phase(ProcessingPhase.VALIDATION)
            self.validate_configuration()
            self._end_phase(ProcessingPhase.VALIDATION)

            self._start_phase(ProcessingPhase.INGESTION)
            preprocessed = self.ingest_document(str(pdf_path))
            self._end_phase(ProcessingPhase.INGESTION)

            self._start_phase(ProcessingPhase.MICRO_EXECUTION)
            micro_results = self.execute_all_micro_questions(preprocessed)
            self._end_phase(ProcessingPhase.MICRO_EXECUTION)

            self._start_phase(ProcessingPhase.SCORING)
            scored_results = self.score_all_questions(micro_results)
            self._end_phase(ProcessingPhase.SCORING)

            self._start_phase(ProcessingPhase.DIMENSION_AGGREGATION)
            dimension_scores = self.aggregate_dimensions(scored_results)
            self._end_phase(ProcessingPhase.DIMENSION_AGGREGATION)

            self._start_phase(ProcessingPhase.AREA_AGGREGATION)
            area_scores = self.aggregate_areas(dimension_scores)
            self._end_phase(ProcessingPhase.AREA_AGGREGATION)

            self._start_phase(ProcessingPhase.CLUSTER_AGGREGATION)
            cluster_scores = self.aggregate_clusters(area_scores)
            self._end_phase(ProcessingPhase.CLUSTER_AGGREGATION)

            self._start_phase(ProcessingPhase.MACRO_EVALUATION)
            macro_score = self.evaluate_macro(cluster_scores)
            self._end_phase(ProcessingPhase.MACRO_EVALUATION)

            self._start_phase(ProcessingPhase.RECOMMENDATIONS)
            recommendations = self.generate_recommendations(
                {
                    "scored_results": scored_results,
                    "dimension_scores": dimension_scores,
                    "area_scores": area_scores,
                    "cluster_scores": cluster_scores,
                    "macro_score": macro_score,
                }
            )
            self._end_phase(ProcessingPhase.RECOMMENDATIONS)

            self._start_phase(ProcessingPhase.REPORT_ASSEMBLY)
            complete_report = self.assemble_report(
                {
                    "micro_results": scored_results,
                    "dimension_scores": dimension_scores,
                    "area_scores": area_scores,
                    "cluster_scores": cluster_scores,
                    "macro_score": macro_score,
                    "recommendations": recommendations,
                }
            )
            self._end_phase(ProcessingPhase.REPORT_ASSEMBLY)

            self._start_phase(ProcessingPhase.OUTPUT_FORMATTING)
            outputs = self.format_outputs(complete_report)
            self._end_phase(ProcessingPhase.OUTPUT_FORMATTING)

            self._set_phase(ProcessingPhase.COMPLETED)
            logger.info("✓ Procesamiento completo para %s", pdf_path)

            return {
                "complete_report": complete_report,
                "output_formats": outputs,
            }
        except Exception as exc:  # pragma: no cover - defensive catch
            logger.exception("Critical failure during process_document: %s", exc)
            self.metrics.errors.append(str(exc))
            self._set_phase(ProcessingPhase.FAILED)
            raise
        finally:
            self.metrics.total_end = time.time()

    def get_processing_status(self) -> Dict[str, Any]:
        """Return a snapshot describing the current execution status."""

        with self._lock:
            elapsed = None
            if self.metrics.total_start is not None:
                elapsed = time.time() - self.metrics.total_start
            timing = self.metrics.phase_timings.get(self.state.current_phase)
            if timing:
                phase_elapsed = time.time() - timing.start_time
            else:
                phase_elapsed = 0.0

            estimated_remaining = None
            if (
                self.state.questions_completed
                and self.metrics.micro_question_durations
            ):
                avg_duration = sum(self.metrics.micro_question_durations) / len(
                    self.metrics.micro_question_durations
                )
                remaining = self.state.questions_total - self.state.questions_completed
                estimated_remaining = max(0.0, remaining * avg_duration)

            total_questions = max(1, self.state.questions_total)
            progress = self.state.questions_completed / total_questions

            return {
                "current_phase": self.state.current_phase.value,
                "progress": progress,
                "questions_completed": self.state.questions_completed,
                "questions_total": self.state.questions_total,
                "elapsed_time": elapsed,
                "current_phase_elapsed": phase_elapsed,
                "estimated_time_remaining": estimated_remaining,
                "errors": list(self.metrics.errors),
                "warnings": list(self.metrics.warnings),
            }

    def get_metrics(self) -> Dict[str, Any]:
        """Expose detailed orchestration metrics for observability."""

        with self._lock:
            total_time = None
            if self.metrics.total_start is not None and self.metrics.total_end is not None:
                total_time = self.metrics.total_end - self.metrics.total_start

            phase_durations = {
                phase.value: timing.duration
                for phase, timing in self.metrics.phase_timings.items()
                if timing.duration is not None
            }

            durations = list(self.metrics.micro_question_durations)
            avg_duration = mean(durations) if durations else None
            p95_duration = None
            parallel_efficiency = None

            micro_timing = self.metrics.phase_timings.get(ProcessingPhase.MICRO_EXECUTION)
            micro_phase_duration = micro_timing.duration if micro_timing else None
            if durations:
                sorted_durations = sorted(durations)
                index = int(0.95 * (len(sorted_durations) - 1))
                p95_duration = sorted_durations[index]

                if (
                    micro_phase_duration
                    and micro_phase_duration > 0
                    and self.config["max_workers"] > 0
                ):
                    total_work = sum(durations)
                    parallel_efficiency = min(
                        1.0,
                        total_work / (micro_phase_duration * self.config["max_workers"]),
                    )

            return {
                "total_time": total_time,
                "phase_durations": phase_durations,
                "micro_question": {
                    "count": len(durations),
                    "avg_duration": avg_duration,
                    "p95_duration": p95_duration,
                    "parallel_efficiency": parallel_efficiency,
                },
                "errors": list(self.metrics.errors),
                "warnings": list(self.metrics.warnings),
            }

    # ------------------------------------------------------------------
    # Phase implementations
    # ------------------------------------------------------------------
    def validate_configuration(self) -> bool:
        """Validate questionnaire and method catalogue integrity."""

        logger.info("Validating configuration integrity…")

        integrity = self.monolith.get("integrity", {})
        stored_hash = integrity.get("monolith_hash")
        if not stored_hash:
            raise RuntimeError("Monolith integrity hash is missing")

        computed_hash = _compute_integrity_hash(self.monolith)
        if computed_hash != stored_hash:
            raise RuntimeError(
                "Monolith hash mismatch: expected %s, computed %s"
                % (stored_hash, computed_hash)
            )

        counts = integrity.get("question_count", {})
        micro_questions = self.monolith.get("blocks", {}).get("micro_questions", [])
        meso_questions = self.monolith.get("blocks", {}).get("meso_questions", [])
        macro_question = self.monolith.get("blocks", {}).get("macro_question")

        if len(micro_questions) != counts.get("micro"):
            raise RuntimeError(
                f"Micro question count mismatch: integrity={counts.get('micro')} actual={len(micro_questions)}"
            )
        if len(meso_questions) != counts.get("meso"):
            raise RuntimeError(
                f"Meso question count mismatch: integrity={counts.get('meso')} actual={len(meso_questions)}"
            )
        if (macro_question is None) != (counts.get("macro", 0) == 0):
            expected_macro = counts.get("macro", 0)
            actual_macro = 0 if macro_question is None else 1
            raise RuntimeError(
                f"Macro question count mismatch: integrity={expected_macro} actual={actual_macro}"
            )

        catalog_metadata_total = (
            self.method_catalog.get("metadata", {}).get("total_methods")
        )
        if catalog_metadata_total != EXPECTED_METHOD_TOTAL:
            raise RuntimeError(
                "Method catalog metadata total mismatch: expected %d found %s"
                % (EXPECTED_METHOD_TOTAL, catalog_metadata_total),
            )

        summary = self.method_catalog.get("summary", {})
        declared_total = summary.get("total_methods")
        if declared_total != EXPECTED_METHOD_TOTAL:
            raise RuntimeError(
                "Method catalog summary total mismatch: expected %d found %s"
                % (EXPECTED_METHOD_TOTAL, declared_total),
            )

        actual_total = sum(
            file_info.get("method_count", 0)
            for file_info in self.method_catalog.get("files", {}).values()
        )
        if actual_total != EXPECTED_METHOD_TOTAL:
            raise RuntimeError(
                "Method catalog file total mismatch: expected %d found %d"
                % (EXPECTED_METHOD_TOTAL, actual_total),
            )

        self._validate_base_slot_mapping(micro_questions)
        self._validate_cluster_assignments()

        logger.info(
            "✓ Monolith y catálogo validados (micro=%d, meso=%d, macro=%s, métodos=%d)",
            len(micro_questions),
            len(meso_questions),
            "1" if macro_question else "0",
            actual_total,
        )

        self.state.questions_total = len(micro_questions)
        return True

    def ingest_document(self, pdf_path: str) -> Any:
        """Delegate the document ingestion pipeline to the DI package."""

        di_module = self._lazy_import("document_ingestion")

        try:
            loader = getattr(di_module, "DocumentLoader")
            text_extractor = getattr(di_module, "TextExtractor")
            preprocessing = getattr(di_module, "PreprocessingEngine")
        except AttributeError as exc:  # pragma: no cover - defensive branch
            raise RuntimeError(
                "document_ingestion module is missing required classes"
            ) from exc

        logger.info("=== FASE 1: INGESTIÓN DEL DOCUMENTO ===")
        raw_document = loader.load_pdf(pdf_path)
        logger.info("✓ PDF cargado")

        _ = text_extractor.extract_full_text(raw_document)
        logger.info("✓ Texto extraído")

        preprocessed = preprocessing.preprocess_document(raw_document)
        logger.info("✓ Documento preprocesado")

        self.state.preprocessed_document = preprocessed
        return preprocessed

    def execute_all_micro_questions(self, preprocessed_doc: Any) -> List[Any]:
        """Execute the 300 micro questions in parallel using coreographers."""

        pool = self._create_choreographer_pool(self.config["max_workers"])
        futures = self._distribute_questions(pool, preprocessed_doc)
        try:
            results = self._wait_for_all_questions(futures)
        finally:
            pool.shutdown()
            with self._lock:
                self.state.active_futures = []

        return results

    def score_all_questions(self, micro_results: List[Any]) -> List[Any]:
        """Apply scoring modalities to the micro question results."""

        scoring_module = self._lazy_import("scoring")
        scorer = getattr(scoring_module, "MicroQuestionScorer")

        scoring_config = self.monolith["blocks"].get("scoring", {})
        scored_results = []
        logger.info("=== FASE 3: SCORING DE MICRO PREGUNTAS ===")
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.config["max_workers"]
        ) as executor:
            future_map = {
                executor.submit(
                    self._score_single_question,
                    scorer,
                    result,
                    scoring_config,
                ): result
                for result in micro_results
            }
            for future in concurrent.futures.as_completed(future_map):
                scored_results.append(future.result())

        self.state.scored_results = scored_results
        return scored_results

    def aggregate_dimensions(self, scored_results: List[Any]) -> List[Any]:
        """Aggregate dimension scores using the aggregation module."""

        aggregation_module = self._lazy_import("aggregation")
        aggregator = getattr(aggregation_module, "DimensionAggregator")

        logger.info("=== FASE 4: AGREGACIÓN POR DIMENSIÓN ===")
        grouped = self._group_scores_by_dimension(scored_results)
        results: List[Any] = []
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.config["max_workers"]
        ) as executor:
            futures = [
                executor.submit(
                    aggregator.aggregate_dimension,
                    dimension_id=dimension_id,
                    area_id=area_id,
                    q_results=q_results,
                    monolith=self.monolith,
                )
                for (area_id, dimension_id), q_results in grouped.items()
            ]
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())

        self.state.dimension_scores = results
        return results

    def aggregate_areas(self, dimension_scores: List[Any]) -> List[Any]:
        """Aggregate area scores."""

        aggregation_module = self._lazy_import("aggregation")
        aggregator = getattr(aggregation_module, "AreaPolicyAggregator")

        logger.info("=== FASE 5: AGREGACIÓN POR ÁREA DE POLÍTICA ===")
        results = []
        grouped = self._group_dimensions_by_area(dimension_scores)
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.config["max_workers"]
        ) as executor:
            futures = [
                executor.submit(
                    aggregator.aggregate_area,
                    area_id,
                    dim_scores,
                    self.monolith,
                )
                for area_id, dim_scores in grouped.items()
            ]
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())

        self.state.area_scores = results
        return results

    def aggregate_clusters(self, area_scores: List[Any]) -> List[Any]:
        """Aggregate cluster (meso) scores."""

        aggregation_module = self._lazy_import("aggregation")
        aggregator = getattr(aggregation_module, "ClusterAggregator")

        logger.info("=== FASE 6: AGREGACIÓN POR CLUSTER ===")
        cluster_definitions = self._get_cluster_definitions()
        results = []
        for cluster_def in cluster_definitions:
            cluster_id = cluster_def.get("cluster_id") or cluster_def.get("id")
            if not cluster_id:
                raise RuntimeError("Cluster definition missing identifier")

            allowed_areas = set(
                cluster_def.get("policy_area_ids")
                or cluster_def.get("legacy_policy_area_ids")
                or cluster_def.get("areas", [])
            )
            cluster_area_scores = [
                score
                for score in area_scores
                if self._extract_area_id(score) in allowed_areas
            ]

            validator = getattr(aggregator, "validate_cluster_hermeticity", None)
            if callable(validator):
                validator(cluster_def, cluster_area_scores)
            else:
                self._assert_cluster_hermeticity(
                    cluster_id, cluster_def, cluster_area_scores
                )

            results.append(
                aggregator.aggregate_cluster(
                    cluster_id=cluster_id,
                    area_scores=cluster_area_scores,
                    cluster_def=cluster_def,
                    monolith=self.monolith,
                )
            )

        self.state.cluster_scores = results
        return results

    def evaluate_macro(self, cluster_scores: List[Any]) -> Any:
        """Evaluate the macro question."""

        aggregation_module = self._lazy_import("aggregation")
        evaluator = getattr(aggregation_module, "MacroEvaluator")

        logger.info("=== FASE 7: EVALUACIÓN MACRO ===")
        macro_result = evaluator.evaluate_macro(cluster_scores, self.monolith)
        self.state.macro_score = macro_result
        return macro_result

    def generate_recommendations(self, all_scores: Dict[str, Any]) -> List[Any]:
        """Generate recommendations using the report assembly subsystem."""

        report_module = self._lazy_import("report_assembly")
        engine = getattr(report_module, "RecommendationEngine")

        logger.info("=== FASE 8: GENERACIÓN DE RECOMENDACIONES ===")
        recommendations = engine.generate_all_recommendations(all_scores)
        self.state.recommendations = recommendations
        return recommendations

    def assemble_report(self, all_data: Dict[str, Any]) -> Any:
        """Assemble the complete report."""

        report_module = self._lazy_import("report_assembly")
        assembler = getattr(report_module, "ReportAssembler")

        logger.info("=== FASE 9: ENSAMBLADO DE REPORTE ===")
        complete_report = assembler.assemble_full_report(all_data)
        self.state.complete_report = complete_report
        return complete_report

    def format_outputs(self, report: Any) -> Dict[str, Any]:
        """Generate output formats (JSON, HTML, PDF, Excel)."""

        report_module = self._lazy_import("report_assembly")
        formatter = getattr(report_module, "ReportFormatter")

        logger.info("=== FASE 10: FORMATEO Y EXPORTACIÓN ===")
        outputs = {
            "json": formatter.format_as_json(report),
            "html": formatter.format_as_html(report),
            "pdf": formatter.format_as_pdf(report),
            "excel": formatter.format_as_excel(report),
        }
        self.state.outputs = outputs
        return outputs

    # ------------------------------------------------------------------
    # Error handling
    # ------------------------------------------------------------------
    def handle_critical_error(self, error: Exception, context: Dict[str, Any]) -> ErrorResponse:
        """Handle critical orchestration errors with doctrine-aligned policy."""

        completed = self.state.questions_completed
        total = self.state.questions_total
        progress = completed / total if total else 0

        if progress < 0.10:
            decision = "ABORT"
        elif progress < 0.50:
            decision = "PARTIAL"
        else:
            decision = "CONTINUE"

        message = f"Critical error during {self.state.current_phase.value}: {error}"
        logger.error(message)
        self.metrics.errors.append(message)

        return ErrorResponse(
            decision=decision,
            reason=message,
            completed_questions=completed,
            total_questions=total,
        )

    def retry_failed_question(self, question_global: int, max_retries: Optional[int] = None) -> Optional[Any]:
        """Retry a failed question execution with exponential backoff."""

        retries = max_retries if max_retries is not None else self.config["max_retries"]
        backoff = self.config.get("retry_backoff_seconds", [0.0, 5.0, 15.0])

        question_metadata = self._get_question_metadata(question_global)
        executor = self._resolve_question_executor(question_metadata)
        if self.state.preprocessed_document is None:
            raise RuntimeError("Preprocessed document missing during retry")

        for attempt in range(retries):
            delay = backoff[min(attempt, len(backoff) - 1)]
            if delay:
                logger.info(
                    "Retrying question %s in %.1f seconds (attempt %d/%d)",
                    question_global,
                    delay,
                    attempt + 1,
                    retries,
                )
                time.sleep(delay)

            try:
                result = self._execute_single_question(
                    executor,
                    question_global,
                    self.state.preprocessed_document,
                )
                logger.info("✓ Retry successful for question %s", question_global)
                return result
            except Exception as exc:  # pragma: no cover - future behaviour
                logger.warning(
                    "Retry %d/%d for question %s failed: %s",
                    attempt + 1,
                    retries,
                    question_global,
                    exc,
                )

        logger.error("All retries failed for question %s", question_global)
        return None

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _start_phase(self, phase: ProcessingPhase) -> None:
        with self._lock:
            self.state.current_phase = phase
            self.metrics.begin_phase(phase)

    def _end_phase(self, phase: ProcessingPhase) -> None:
        with self._lock:
            self.metrics.end_phase(phase)

    def _set_phase(self, phase: ProcessingPhase) -> None:
        with self._lock:
            self.state.current_phase = phase

    def _lazy_import(self, module_name: str):
        try:
            return importlib.import_module(module_name)
        except ModuleNotFoundError as exc:  # pragma: no cover - defensive branch
            raise RuntimeError(
                f"Required module '{module_name}' is not available. "
                "Ensure the component has been implemented before running the orchestrator."
            ) from exc

    def _validate_base_slot_mapping(self, micro_questions: List[Dict[str, Any]]) -> None:
        expected_slots = {
            f"D{dimension}-Q{question}"
            for dimension in range(1, 7)
            for question in range(1, 6)
        }

        slot_counts: Counter[str] = Counter()
        for question in micro_questions:
            question_global = question.get("question_global")
            base_slot = question.get("base_slot")
            if question_global is None:
                raise RuntimeError("Micro question missing 'question_global'")
            if base_slot is None:
                raise RuntimeError(
                    f"Micro question {question_global} is missing base_slot"
                )

            expected_slot = self._compute_expected_base_slot(int(question_global))
            if base_slot != expected_slot:
                raise RuntimeError(
                    "Base slot mismatch for question %s: expected %s found %s"
                    % (question_global, expected_slot, base_slot)
                )

            slot_counts[base_slot] += 1

        if set(slot_counts.keys()) != expected_slots:
            raise RuntimeError(
                "Base slot coverage mismatch: expected %d slots found %d"
                % (len(expected_slots), len(slot_counts)),
            )

        for slot, count in slot_counts.items():
            if count != 10:
                raise RuntimeError(
                    "Base slot %s has %d questions, expected 10" % (slot, count)
                )

        logger.info("  - Base slots verificados (%d × 10)", len(slot_counts))

    def _validate_cluster_assignments(self) -> None:
        cluster_definitions = self._get_cluster_definitions()
        if not cluster_definitions:
            raise RuntimeError("Monolith is missing cluster definitions")

        policy_area_ownership: Dict[str, str] = {}
        for cluster in cluster_definitions:
            cluster_id = cluster.get("cluster_id") or cluster.get("id")
            if not cluster_id:
                raise RuntimeError("Cluster definition missing identifier")

            policy_areas = cluster.get("policy_area_ids") or cluster.get(
                "legacy_policy_area_ids", []
            )
            if not policy_areas:
                raise RuntimeError(
                    f"Cluster {cluster_id} does not declare policy areas"
                )

            for area_id in policy_areas:
                previous_owner = policy_area_ownership.setdefault(area_id, cluster_id)
                if previous_owner != cluster_id:
                    raise RuntimeError(
                        "Policy area %s assigned to clusters %s and %s"
                        % (area_id, previous_owner, cluster_id)
                    )

        policy_area_defs = self.monolith["blocks"]["niveles_abstraccion"].get(
            "policy_areas", []
        )
        declared_areas = {
            area.get("policy_area_id")
            or (area.get("legacy_ids") or [None])[0]
            for area in policy_area_defs
        }
        declared_areas.discard(None)
        if declared_areas - set(policy_area_ownership.keys()):
            missing = declared_areas - set(policy_area_ownership.keys())
            raise RuntimeError(
                "Policy areas without cluster assignment: %s"
                % ", ".join(sorted(missing))
            )

        logger.info(
            "  - Hermeticidad de clusters verificada (%d policy areas)",
            len(policy_area_ownership),
        )

    @staticmethod
    def _compute_expected_base_slot(question_global: int) -> str:
        base_index = (question_global - 1) % 30
        dimension_index = base_index // 5 + 1
        question_index = base_index % 5 + 1
        return f"D{dimension_index}-Q{question_index}"

    def _get_cluster_definitions(self) -> List[Dict[str, Any]]:
        clusters = self.monolith["blocks"].get("niveles_abstraccion", {}).get(
            "clusters", []
        )
        if isinstance(clusters, dict):
            return list(clusters.values())
        return list(clusters)

    def _assert_cluster_hermeticity(
        self, cluster_id: str, cluster_def: Dict[str, Any], area_scores: List[Any]
    ) -> None:
        allowed_areas = set(
            cluster_def.get("policy_area_ids")
            or cluster_def.get("legacy_policy_area_ids")
            or cluster_def.get("areas", [])
        )
        if not allowed_areas:
            raise RuntimeError(
                f"Cluster {cluster_id} does not declare policy areas for validation"
            )

        score_areas = {self._extract_area_id(score) for score in area_scores}
        score_areas.discard(None)
        if not score_areas.issubset(allowed_areas):
            offending = score_areas - allowed_areas
            raise RuntimeError(
                "Cluster %s received scores for foreign policy areas: %s"
                % (cluster_id, ", ".join(sorted(offending))),
            )

    @staticmethod
    def _extract_area_id(area_score: Any) -> Optional[str]:
        if hasattr(area_score, "area_id"):
            return getattr(area_score, "area_id")
        if isinstance(area_score, dict):
            return (
                area_score.get("area_id")
                or area_score.get("policy_area")
                or area_score.get("policy_area_id")
            )
        return None

    @staticmethod
    def _tag_successful_result(result: Any) -> Any:
        if isinstance(result, dict):
            result.setdefault("status", "SUCCESS")
        return result

    @staticmethod
    def _tag_successful_retry(result: Any) -> Any:
        if isinstance(result, dict):
            result["status"] = "RETRY_SUCCESS"
        return result

    def _build_failed_result(
        self, question_metadata: Dict[str, Any], error_message: str
    ) -> Dict[str, Any]:
        return {
            "question_global": question_metadata.get("question_global"),
            "base_slot": question_metadata.get("base_slot"),
            "policy_area": question_metadata.get("policy_area")
            or question_metadata.get("policy_area_id"),
            "policy_area_id": question_metadata.get("policy_area_id"),
            "dimension": question_metadata.get("dimension")
            or question_metadata.get("dimension_id"),
            "status": "FAILED",
            "error": error_message,
            "evidence": None,
            "raw_results": None,
        }

    def _build_timeout_result(self, question_metadata: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "question_global": question_metadata.get("question_global"),
            "base_slot": question_metadata.get("base_slot"),
            "policy_area": question_metadata.get("policy_area")
            or question_metadata.get("policy_area_id"),
            "policy_area_id": question_metadata.get("policy_area_id"),
            "dimension": question_metadata.get("dimension")
            or question_metadata.get("dimension_id"),
            "status": "TIMEOUT",
            "error": "Micro question execution exceeded allotted time",
            "evidence": None,
            "raw_results": None,
        }

    def _create_choreographer_pool(self, max_workers: int) -> ChoreographerPool:
        logger.info("Creando pool de Coreógrafos con %d workers", max_workers)
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        return ChoreographerPool(executor=executor, max_workers=max_workers)

    def _distribute_questions(
        self, pool: ChoreographerPool, preprocessed_doc: Any
    ) -> Dict[concurrent.futures.Future, Dict[str, Any]]:
        futures: Dict[concurrent.futures.Future, Dict[str, Any]] = {}
        logger.info("Distribuyendo 300 micro preguntas a los Coreógrafos")

        for question in self.monolith["blocks"]["micro_questions"]:
            question_global = question["question_global"]
            executor = self._resolve_question_executor(question)
            future = pool.executor.submit(
                self._execute_single_question,
                executor,
                question_global,
                preprocessed_doc,
            )
            futures[future] = question

        with self._lock:
            self.state.active_futures = list(futures.keys())
        return futures

    def _wait_for_all_questions(
        self, futures: Dict[concurrent.futures.Future, Dict[str, Any]]
    ) -> List[Any]:
        timeout = self.config.get("micro_question_timeout")
        logger.info("Esperando la finalización de todas las preguntas (timeout=%s)", timeout)

        start_time = time.time()
        deadline = start_time + timeout if timeout is not None else None
        pending = dict(futures)
        results: List[Any] = []
        completed = 0

        while pending:
            wait_timeout: Optional[float] = None
            if deadline is not None:
                remaining = deadline - time.time()
                if remaining <= 0:
                    break
                wait_timeout = remaining

            done, _ = concurrent.futures.wait(
                pending.keys(),
                timeout=wait_timeout,
                return_when=concurrent.futures.FIRST_COMPLETED,
            )

            if not done:
                break

            for future in done:
                question_metadata = pending.pop(future)
                question_global = question_metadata["question_global"]
                try:
                    result = future.result()
                except Exception as exc:  # pragma: no cover - relies on components
                    logger.warning(
                        "Pregunta %s falló: %s. Iniciando reintentos locales",
                        question_global,
                        exc,
                    )
                    self.metrics.errors.append(str(exc))
                    try:
                        retry_result = self.retry_failed_question(question_global)
                    except Exception as retry_exc:  # pragma: no cover - defensive
                        logger.error(
                            "Reintentos para la pregunta %s fallaron con excepción: %s",
                            question_global,
                            retry_exc,
                        )
                        self.metrics.errors.append(str(retry_exc))
                        retry_result = None

                    if retry_result is not None:
                        results.append(self._tag_successful_retry(retry_result))
                    else:
                        failure_payload = self._build_failed_result(
                            question_metadata, str(exc)
                        )
                        results.append(failure_payload)
                        self.metrics.errors.append(
                            f"Micro question {question_global} failed after retries"
                        )
                else:
                    results.append(self._tag_successful_result(result))

                completed += 1
                with self._lock:
                    self.state.questions_completed = completed

                now = time.time()
                if now - self.state.last_progress_update >= self.config["status_interval"]:
                    logger.info(
                        "Progreso: %d/%d preguntas completadas",
                        completed,
                        self.state.questions_total,
                    )
                    self.state.last_progress_update = now

        if pending:
            if timeout is not None:
                logger.error(
                    "Timeout alcanzado tras %.1f segundos esperando micro preguntas",
                    timeout,
                )
            for future, metadata in pending.items():
                future.cancel()
                timeout_result = self._build_timeout_result(metadata)
                results.append(timeout_result)
                self.metrics.warnings.append(
                    f"Micro question {metadata['question_global']} timed out"
                )

            completed += len(pending)
            with self._lock:
                self.state.questions_completed = completed

        if completed != self.state.questions_total:
            logger.warning(
                "Solo %d de %d preguntas completaron dentro del timeout",
                completed,
                self.state.questions_total,
            )

        results.sort(key=lambda item: self._safe_get(item, "question_global") or 0)
        return results

    def _execute_single_question(
        self,
        executor: QuestionExecutor,
        question_global: int,
        preprocessed_doc: Any,
    ) -> Any:
        start = time.time()
        result = executor(
            question_global,
            preprocessed_doc,
            self.monolith,
            self.method_catalog,
        )
        duration = time.time() - start
        self.metrics.record_micro_duration(duration)
        return result

    def _score_single_question(
        self,
        scorer: Any,
        micro_result: Any,
        scoring_config: Dict[str, Any],
    ) -> Any:
        question_global = self._safe_get(micro_result, "question_global")
        if question_global is None:
            raise ValueError("Micro result is missing 'question_global'")
        question_index = int(question_global) - 1
        question_metadata = self.monolith["blocks"]["micro_questions"][question_index]
        modality = question_metadata["scoring_modality"]

        score = scorer.apply_scoring_modality(
            evidence=self._safe_get(micro_result, "evidence"),
            modality=modality,
            config=scoring_config,
        )

        quality = scorer.determine_quality_level(
            score=score,
            thresholds=scoring_config.get("quality_levels", {}),
        )

        builder = getattr(scorer, "build_scored_result", None)
        if callable(builder):
            return builder(
                micro_result=micro_result,
                score=score,
                quality_level=quality,
                metadata=question_metadata,
            )

        return {
            "question_global": self._safe_get(micro_result, "question_global"),
            "base_slot": question_metadata.get("base_slot"),
            "policy_area": question_metadata.get("policy_area"),
            "dimension": question_metadata.get("dimension"),
            "score": score,
            "quality_level": quality,
            "evidence": self._safe_get(micro_result, "evidence"),
            "raw_results": self._safe_get(micro_result, "raw_results"),
        }

    def _resolve_question_executor(self, question_metadata: Dict[str, Any]) -> QuestionExecutor:
        if self._question_executor_factory is not None:
            return self._question_executor_factory(question_metadata)

        if self._coreographer_cls is None:
            try:
                coreographer_module = importlib.import_module("coreographer")
                self._coreographer_cls = getattr(coreographer_module, "Coreographer")
            except (ModuleNotFoundError, AttributeError) as exc:  # pragma: no cover
                raise RuntimeError(
                    "Coreographer module is required but not available."
                ) from exc

        coreographer_instance = self._coreographer_cls(
            question_metadata=question_metadata,
            monolith=self.monolith,
            method_catalog=self.method_catalog,
        )

        if not hasattr(coreographer_instance, "execute"):
            raise RuntimeError("Coreographer instance does not implement 'execute'")

        return coreographer_instance.execute  # type: ignore[return-value]

    def _list_policy_areas(self) -> List[str]:
        areas = self.monolith["blocks"].get("niveles_abstraccion", {}).get("policy_areas", {})
        return list(areas.keys())

    def _get_question_metadata(self, question_global: int) -> Dict[str, Any]:
        index = question_global - 1
        questions = self.monolith["blocks"]["micro_questions"]
        if index < 0 or index >= len(questions):  # pragma: no cover - bounds safety
            raise IndexError(f"Invalid question_global: {question_global}")
        return questions[index]

    def _group_scores_by_dimension(
        self, scored_results: List[Any]
    ) -> Dict[Tuple[str, str], List[Any]]:
        grouped: Dict[Tuple[str, str], List[Any]] = {}
        for result in scored_results:
            area_id = self._safe_get(result, "policy_area")
            dimension_id = self._safe_get(result, "dimension")
            if area_id is None or dimension_id is None:
                continue
            key = (area_id, dimension_id)
            grouped.setdefault(key, []).append(result)
        return grouped

    def _group_dimensions_by_area(
        self, dimension_scores: List[Any]
    ) -> Dict[str, List[Any]]:
        grouped: Dict[str, List[Any]] = {}
        for dim_score in dimension_scores:
            area_id = self._safe_get(dim_score, "area_id")
            if area_id is None:
                continue
            grouped.setdefault(area_id, []).append(dim_score)
        return grouped

    @staticmethod
    def _safe_get(obj: Any, attribute: str) -> Any:
        if hasattr(obj, attribute):
            return getattr(obj, attribute)
        if isinstance(obj, dict):
            return obj.get(attribute)
        return None


__all__ = [
    "ProcessingPhase",
    "ProcessingMetrics",
    "ProcessingState",
    "ErrorResponse",
    "Orchestrator",
]

